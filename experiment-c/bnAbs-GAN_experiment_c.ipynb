{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bnAbs-GAN_experiment-c.ipynb","provenance":[],"collapsed_sections":["JrF9RPhYWcdb","-uMNzV06AzNj","B7E7eH8tDyVn","sWO12qslt1dh","E7kGUnRnCReJ","r91jJ0Sb6NnD","LzuN_7U6RDhJ","-DJ9CmwmTWTx","QMgdcfmLww6b"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JrF9RPhYWcdb"},"source":["# Set up the environment"]},{"cell_type":"code","metadata":{"id":"pE6psVsXWhaB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682562542,"user_tz":-120,"elapsed":120095,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"d64ccd00-def9-4515-cd77-22c820dfcca9"},"source":["# NOTE: after 'Restart runtime', installed packages are maintained!\n","!pip install transformers\n","!pip install tape_proteins # Takes a while\n","!pip install torchmetrics\n","\n","!pip install pickle5\n","!pip install fastaparser"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 4.2MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 48.7MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 55.6MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n","Collecting tape_proteins\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f7/bdfe0ef6fd6ffb45f55c944176f518b0bc6ea0c9dddba0816578fb0e7290/tape_proteins-0.4-py3-none-any.whl (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n","\u001b[?25hCollecting biopython\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/42/de1ed545df624180b84c613e5e4de4848f72989ce5846a74af6baa0737b9/biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 7.5MB/s \n","\u001b[?25hCollecting torch<1.5,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n","\u001b[?25hCollecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/94/4774265ce7a53c38f93e062e8b5ffea3f0ec1efde9e8d91d81e2d6ccd7e8/boto3-1.17.93-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 56.2MB/s \n","\u001b[?25hCollecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 58.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tape_proteins) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tape_proteins) (4.41.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tape_proteins) (1.4.1)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from tape_proteins) (0.99)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython->tape_proteins) (1.19.5)\n","Collecting botocore<1.21.0,>=1.20.93\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/fe/0348a318ee4f9f0240bad0171c5384d68c30de4d6b1e9a40464c59ed927d/botocore-1.20.93-py2.py3-none-any.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 43.2MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.5.0,>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->tape_proteins) (3.12.4)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tape_proteins) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tape_proteins) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tape_proteins) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tape_proteins) (2.10)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.93->boto3->tape_proteins) (2.8.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->tape_proteins) (57.0.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->tape_proteins) (1.15.0)\n","\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: botocore 1.20.93 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: biopython, torch, jmespath, botocore, s3transfer, boto3, tensorboardX, tape-proteins\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","Successfully installed biopython-1.79 boto3-1.17.93 botocore-1.20.93 jmespath-0.10.0 s3transfer-0.4.2 tape-proteins-0.4 tensorboardX-2.2 torch-1.4.0\n","Collecting torchmetrics\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e8/513cd9d0b1c83dc14cd8f788d05cd6a34758d4fd7e4f9e5ecd5d7d599c95/torchmetrics-0.3.2-py3-none-any.whl (274kB)\n","\u001b[K     |████████████████████████████████| 276kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (20.9)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.4.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (2.4.7)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.3.2\n","Collecting pickle5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n","\u001b[K     |████████████████████████████████| 133kB 4.2MB/s \n","\u001b[?25hBuilding wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219278 sha256=baa7062394fa881b4549123a94609d3d5aa331ebbbaae3208a0ce17cc0e43a83\n","  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n","Collecting fastaparser\n","  Downloading https://files.pythonhosted.org/packages/05/0a/fbab5288782d700d04375b8227800dd8cbc176e9f5327bea4e336da579fd/fastaparser-1.1-py3-none-any.whl\n","Installing collected packages: fastaparser\n","Successfully installed fastaparser-1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AyifetxOWkTe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682565142,"user_tz":-120,"elapsed":2604,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"a41fcd7e-1850-4b5b-ef43-f2728fe355df"},"source":["import torch\n","import torch.nn as nn\n","\n","import time\n","import numpy as np\n","import random\n","import csv\n","import os\n","import os.path\n","import pandas as pd\n","import math\n","import sys\n","import editdistance\n","import torchmetrics\n","import matplotlib.pyplot as plt\n","\n","from torchmetrics import Accuracy, Precision, Recall, F1\n","from transformers import get_polynomial_decay_schedule_with_warmup\n","from torch.utils.data import Dataset, DataLoader\n","from tape import ProteinBertModel, TAPETokenizer\n","\n","import pickle5 as pickle\n","import fastaparser, gzip, shutil\n","\n","# Some helper methods\n","def format_time(t):\n","    return time.strftime('%H:%M:%S', time.gmtime(t))\n","\n","def save_obj(obj, name):\n","    with open(name + '.pkl', 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","def load_obj(name):\n","    with open(name + '.pkl', 'rb') as f:\n","        return pickle.load(f)\n","\n","# Returns the number of clusters and clusters as dict\n","def cluster(cdr3_list, cluster_identity=0.70): # cdr3_list = list of tuples (id, cdr3)\n","    os.system('wget https://drive5.com/downloads/usearch11.0.667_i86linux32.gz -O usearch.gz')\n","    with gzip.open('usearch.gz', 'rb') as f_in:\n","        with open('usearch', 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","    os.remove('usearch.gz')\n","    os.system('chmod +rwx usearch') # Change access permission\n","\n","    with open('cdr3.fasta', 'w') as fasta_file:\n","        writer = fastaparser.Writer(fasta_file)\n","        for seq_id, cdr3 in cdr3_list:\n","            writer.writefasta((seq_id, cdr3))\n","\n","    os.mkdir(os.getcwd()+'/usearch_results')\n","    os.system(os.getcwd()+'/usearch -cluster_fast cdr3.fasta -id '+str(cluster_identity)+' -clusters usearch_results/nr.fasta')\n","    os.system(os.getcwd()+'/usearch -cluster_fast cdr3.fasta -id '+str(cluster_identity)+' -centroids centroids.fasta')\n","    \n","    res = dict(); count = 0\n","    with open('centroids.fasta') as fasta_file: # centroids\n","        parser = fastaparser.Reader(fasta_file)\n","        for i, seq in enumerate(parser):\n","            with open('usearch_results/nr.fasta'+str(i)) as fasta_file_cluster:\n","                parser_cluster = fastaparser.Reader(fasta_file_cluster)\n","                for cls_seq in parser_cluster:\n","                    res.setdefault(count,[]).append((cls_seq.id))\n","                count += 1\n","    \n","    shutil.rmtree(os.getcwd()+'/usearch_results', ignore_errors=True) # remove intermediate results\n","    os.remove('centroids.fasta')\n","    os.remove('cdr3.fasta')\n","    os.remove('usearch')\n","    return count, res\n","\n","# Set random values to make it reproducible\n","seed_val = 0\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","\n","# Make sure that runtime type is GPU\n","if torch.cuda.is_available():\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device('cuda')\n","    torch.cuda.manual_seed_all(seed_val)\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0),'\\n')\n","else:\n","    print('No GPU available, using the CPU instead.\\n')\n","    device = torch.device('cpu')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VWopir9Zw69P"},"source":["# Create train and test dataset\n"]},{"cell_type":"markdown","metadata":{"id":"-uMNzV06AzNj"},"source":["## Load datasets"]},{"cell_type":"markdown","metadata":{"id":"y8rRaPJ8GV0u"},"source":["To summerize:\n","\n","* ``` best_clusters_dict ```: mapping from cluster to centroid and containing ids\n","* ``` best_dict ```: mapping from ids of best neutralizing antibodies to all relevant information. Size: 68\n","* ``` broad-neutralizing_dict ```\n","* ``` non-neutralizing_dict ```\n","* ``` UZH_dataset_dict ```\n","* ``` simonich_dict ```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y60E0BlJETuV","executionInfo":{"status":"ok","timestamp":1623682596164,"user_tz":-120,"elapsed":17438,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"6690fa60-0ae7-4b47-d16d-778ae24586ca"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0R5aJuPaR85e","executionInfo":{"status":"ok","timestamp":1623682608752,"user_tz":-120,"elapsed":12591,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["datasets_dir = '/content/drive/MyDrive/datasets/' # From where to load datasets. I used google drive.\n","\n","best_clusters_dict = load_obj(datasets_dir+'best_clusters_dict')\n","best_dict = load_obj(datasets_dir+'best_dict')\n","\n","broad_dict = load_obj(datasets_dir+'broad-neutralizing_dict')\n","non_dict = load_obj(datasets_dir+'non-neutralizing_dict')\n","UZH_dict = load_obj(datasets_dir+'UZH_dataset_dict')\n","simonich_dict = load_obj(datasets_dir+'simonich_dict')\n","\n","# These are datasets for which the split based on the germline identity will be performed\n","ds_list = [('broad',broad_dict),('non',non_dict),('UZH',UZH_dict),('simonich',simonich_dict)]"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7E7eH8tDyVn"},"source":["## Split according to germline identity"]},{"cell_type":"code","metadata":{"id":"1fdTOKu8zzU5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682609901,"user_tz":-120,"elapsed":1160,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"4175d5a5-8171-4c35-bb40-e036accf61a5"},"source":["# Split data sets into sequences below and above the v_j_identity_threshold\n","v_j_identity_threshold = 85\n","\n","dicts_low = dict() # Used for training\n","dicts_high = dict()\n","for lab, d in ds_list:\n","    dicts_low[lab] = dict()\n","    dicts_high[lab] = dict()\n","    for k in d.keys():\n","        v_identity = float(d[k]['v_identity'])\n","        j_identity = float(d[k]['j_identity'])\n","\n","        if (v_identity + j_identity)/2 < v_j_identity_threshold:\n","            dicts_low[lab][k] = d[k]\n","        else:\n","            dicts_high[lab][k] = d[k]\n","    print('{}\\t | Number of sequences below v-j-identity threshold {} \\t(total = {})'.format(lab,len(dicts_low[lab]),len(d)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["broad\t | Number of sequences below v-j-identity threshold 7579 \t(total = 409622)\n","non\t | Number of sequences below v-j-identity threshold 1723 \t(total = 384028)\n","UZH\t | Number of sequences below v-j-identity threshold 373 \t(total = 12664)\n","simonich\t | Number of sequences below v-j-identity threshold 5 \t(total = 202166)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sWO12qslt1dh"},"source":["## Reserve sequences for testing"]},{"cell_type":"code","metadata":{"id":"EhClV1Z-txhI","executionInfo":{"status":"ok","timestamp":1623682613919,"user_tz":-120,"elapsed":4019,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["test_sizes = {\n","    'low': {\n","        'broad': 500,\n","        'non': 300, # Notice that 14 will be used in the validation set\n","        'UZH': 100,\n","        'simonich': 5 # Not used during training\n","    },\n","    'high': {\n","        'broad': 10000,\n","        'non': 10000,\n","        'UZH': 10000,\n","        'simonich': 10000\n","    },\n","}\n","dicts_low_test = dict()\n","dicts_high_test = dict()\n","for lab, _ in ds_list:\n","    dicts_low_test[lab] = dict(list(dicts_low[lab].items())[:test_sizes['low'][lab]])\n","    dicts_low[lab] = dict(list(dicts_low[lab].items())[test_sizes['low'][lab]:])\n"," \n","    dicts_high_test[lab] = dict(list(dicts_high[lab].items())[:test_sizes['high'][lab]])\n","    dicts_high[lab]= dict(list(dicts_high[lab].items())[test_sizes['high'][lab]:])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7kGUnRnCReJ"},"source":["## Visualize datasets"]},{"cell_type":"code","metadata":{"id":"BF04A-XYCRHW","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1623682614969,"user_tz":-120,"elapsed":1053,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"96f7de0c-d3ba-4367-835d-e68ae7f0cb81"},"source":["dpi = 96*2\n","plt.figure(figsize=(8,4),dpi=dpi)\n","\n","dataset = 'broad'\n","dict_ = dicts_high\n","\n","data = dict_[dataset]\n","x = [len(y['cdr3_aa']) for _,y in data.items()]\n","n_bins = max(x)-min(x)+1\n","n, bins = np.histogram(x, bins=n_bins)\n","\n","plt.bar([min(x)+y for y in range(n_bins)],n,log=True)\n","\n","plt.xlabel('length of cdr3 region')\n","plt.ylabel('count')\n","# plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n","# plt.savefig(destination_folder+'HISTOGRAM_'+dataset+'_.png',transparent=True,dpi=dpi,pad_inches=0.1,bbox_inches='tight')\n","plt.show()"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1536x768 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"r91jJ0Sb6NnD"},"source":["## Define similarity functions"]},{"cell_type":"markdown","metadata":{"id":"VXMcyfd26eAK"},"source":["Guidelines:\n","\n","* bnAbs training & testing as different as possible. First the sequence similarity should be as low as possible, then if possible different germline sequences. Similarity is defined as the weighted average of the cdr3 similarity and the germline similarity:\n","\n","$$ sim_{id}(id_1,id_2):= \\alpha\\cdot sim(id_1.cdr3,id_2.cdr3)+(1-\\alpha)\\cdot sim(id_1.ger,id_2.ger) $$ \n","\n","$$ sim(a,b):= 1-\\frac{edit\\_distance(a,b)}{\\max\\{len(a),len(b)\\}} $$ \n","\n","$$0\\leq\\alpha\\leq 1$$\n","\n","* labelled bnAbs & non-bnAbs sequences of the training set should be as similar as possible, i.e. same germeline sequences and ideally same CDRH3 length and sequence identity to germline"]},{"cell_type":"code","metadata":{"id":"H85xZ3Ko6XHk","executionInfo":{"status":"ok","timestamp":1623682614970,"user_tz":-120,"elapsed":7,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["def similarity(key_a, key_b, dict_a, dict_b, alpha):\n","    cdr3_1 = dict_a[key_a]['cdr3_aa']\n","    cdr3_2 = dict_b[key_b]['cdr3_aa']\n","    ga_1 = dict_a[key_a]['germline_alignment_aa']\n","    ga_2 = dict_b[key_b]['germline_alignment_aa']\n","    sim_cdr3 = 1 - (editdistance.eval(cdr3_1, cdr3_2) / max(len(cdr3_1), len(cdr3_2)))\n","    sim_ga = 1 - (editdistance.eval(ga_1, ga_2) / max(len(ga_1), len(ga_2)))\n","    return alpha * sim_cdr3 + (1-alpha) * sim_ga\n","\n","# Implemented as in the report\n","# most=False -> find LESS similar set\n","def find_most_similar(given_dict, search_dict, out_size, alpha, verbose=False, most=True):\n","    out_set = list(); round_robin = False\n","    search_keys = list(search_dict.keys())\n","    given_keys = list(given_dict.keys())\n","    random.shuffle(given_keys)\n","    random.shuffle(search_keys)\n","    interval_size_search = int(len(search_keys)/out_size)\n","    interval_size_given = int(len(given_keys)/out_size)\n","    if interval_size_given == 0:\n","        round_robin = True\n","    for i in range(out_size):\n","        if i < out_size - 1:\n","            search_keys_int = search_keys[interval_size_search*i:interval_size_search*(i+1)]\n","            given_keys_int = [given_keys[i%len(given_keys)]] if round_robin else given_keys[interval_size_given*i:interval_size_given*(i+1)]\n","        else: # last intervals take everything unseen\n","            search_keys_int = search_keys[interval_size_search*i:]\n","            given_keys_int = [given_keys[i%len(given_keys)]] if round_robin else given_keys[interval_size_given*i:]\n","\n","        best_sim = sys.float_info.min if most else sys.float_info.max\n","        best_key = None\n","        for search_key in search_keys_int:\n","            curr_sim = 0\n","            for given_key in given_keys_int:\n","                curr_sim += similarity(given_key, search_key, given_dict, search_dict, alpha)\n","            curr_sim /= len(given_keys_int) # Average\n","            if most and curr_sim > best_sim:\n","                best_sim = curr_sim\n","                best_key = search_key\n","            elif not most and curr_sim < best_sim:\n","                best_sim = curr_sim\n","                best_key = search_key\n","        out_set.append(best_key)\n","        if verbose:\n","            sys.stdout.write('\\rSearch progress: {0} %'.format(int(100*(i+1)/out_size)))\n","            sys.stdout.flush()\n","    if verbose:\n","        print('\\n')\n","    return out_set\n","\n","def check_similarity(keys_a, keys_b, dict_a, dict_b, alpha):\n","    max_sim = sys.float_info.min\n","    min_sim = sys.float_info.max\n","    cum_sim = 0\n","    for key_a in keys_a:\n","        for key_b in keys_b:\n","            sim = similarity(key_a, key_b, dict_a, dict_b, alpha)\n","            cum_sim += sim\n","            if sim > max_sim:\n","                max_sim = sim\n","            if sim < min_sim:\n","                min_sim = sim\n","    avg_sim = cum_sim/(len(keys_a)*len(keys_b))\n","    return avg_sim, min_sim, max_sim"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzuN_7U6RDhJ"},"source":["## CREATE TRAINING DATA"]},{"cell_type":"markdown","metadata":{"id":"1p7xckTahoJe"},"source":["1. Define parameters"]},{"cell_type":"code","metadata":{"id":"SMEsAnrvhoJv","executionInfo":{"status":"ok","timestamp":1623682614971,"user_tz":-120,"elapsed":7,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["train_ratio = 0.8\n","alpha = 0.3\n","file_name_train = 'TRAIN_SET_baseline+UZH_labeled_non'\n","\n","UZH_train_size_lab = 100 # Number of UZH seqs. with high germ. id. destined for training"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LoGz4bQhoJw"},"source":["2. Labeled data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gfhid8UDhoJx","executionInfo":{"status":"ok","timestamp":1623682615444,"user_tz":-120,"elapsed":479,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"067cfe1a-b368-4a13-9f6f-a4e29b3d4f0a"},"source":["size_best = len(best_dict)\n","num_clusters = len(best_clusters_dict)\n","\n","# Greedy algorithm to compute train set (i.e. set of clusters) for pos. instances ONLY based on similarity and train_ratio\n","# Start with first cluster and then in each step add cluster that on average is most similar to already added sequences\n","# Hopefully this results in very different train and test sets\n","# This is identical to find_most_similar adapted to clusters (best broad neutr.)\n","R = int(size_best * train_ratio); fst_cls = random.randint(0, num_clusters-1)\n","train_clusters = {fst_cls}\n","size_train_keys_best = len(best_clusters_dict[fst_cls]['set'])\n","remaining_clusters = set(range(0, num_clusters)).difference(train_clusters)\n","while size_train_keys_best < R:\n","    best_score = sys.float_info.min\n","    best_cluster = None\n","    for r_c in remaining_clusters:\n","        cum_cluster_score = 0\n","        num_compared_seqs = 0\n","        for t_c in train_clusters:\n","            for key_b_c in best_clusters_dict[t_c]['set']:\n","                for key_r_c in best_clusters_dict[r_c]['set']:\n","\n","                    sim = similarity(key_b_c, key_r_c, best_dict, best_dict, alpha)\n","\n","                    cum_cluster_score += sim\n","                    num_compared_seqs += 1\n","        \n","        cluster_score = cum_cluster_score / num_compared_seqs\n","\n","        if cluster_score > best_score:\n","            best_cluster = r_c\n","            best_score = cluster_score\n","    \n","    size_train_keys_best += len(best_clusters_dict[best_cluster]['set'])\n","    train_clusters.add(best_cluster)\n","    remaining_clusters.remove(best_cluster)\n","\n","test_clusters = remaining_clusters\n","size_test_keys_best = size_best - size_train_keys_best\n","\n","assert(len(train_clusters)+len(test_clusters) == num_clusters)\n","print('Split of best-broad-neutr. seqs: (train, test) = ({}, {})\\n'.format(size_train_keys_best,size_test_keys_best))\n","\n","# Check similarity between positive instances of train and test set\n","train_keys_best = [y for x in train_clusters for y in best_clusters_dict[x]['set']]\n","test_keys_best = [y for x in test_clusters for y in best_clusters_dict[x]['set']]\n","assert(len(train_keys_best) == size_train_keys_best and len(test_keys_best) == size_test_keys_best)\n","avg_sim, min_sim, max_sim = check_similarity(train_keys_best, test_keys_best, best_dict, best_dict, alpha)\n","print('Similarity between positive instances of train and test set: max =',max_sim,', avg =',avg_sim,'\\n')\n","\n","# Compute set of keys for negative instances of train set (most similar to positives ones)\n","given_dict = {k:v for k,v in best_dict.items() if k in train_keys_best}\n","train_keys_non_lab = find_most_similar(given_dict, dicts_low['non'], size_train_keys_best, alpha)\n","# Check similarity between positive and negative instances of train set\n","avg_sim, min_sim, max_sim = check_similarity(train_keys_best, train_keys_non_lab, best_dict, dicts_low['non'], alpha)\n","print('Similarity between positive and negative instances of train set: min =',min_sim,', avg =',avg_sim,'\\n')\n","\n","# Add random UZH sequences with high germline identity\n","UZH_high_keys = list(dicts_high['UZH'].keys()).copy()\n","random.shuffle(UZH_high_keys)\n","train_keys_UZH_lab = UZH_high_keys[:min(len(UZH_high_keys),UZH_train_size_lab)]"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Split of best-broad-neutr. seqs: (train, test) = (54, 14)\n","\n","Similarity between positive instances of train and test set: max = 0.5244730679156909 , avg = 0.39617916348741355 \n","\n","Similarity between positive and negative instances of train set: min = 0.22019230769230771 , avg = 0.4896866296258902 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"04obEnCKhoJy"},"source":["3. Unlabeled data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dy9JjNdRhoJy","executionInfo":{"status":"ok","timestamp":1623682615444,"user_tz":-120,"elapsed":19,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"24d368ca-b2f6-4e0d-96b6-4f01781524fc"},"source":["# Remove those already used in labeled section\n","unused_keys_non = set(dicts_low['non'].keys()).difference(train_keys_non_lab)\n","\n","# GAN-BERT paper says |U| = 100 * |L|\n","# size_unlab_goal = 100*2*size_train_keys_best\n","\n","# Non-neutralizing sequences\n","train_keys_non_unlab = unused_keys_non\n","print('Number of non-neutr. sequences in train set (unlabeled) = {}'.format(len(train_keys_non_unlab)))\n","\n","# UZH sequences\n","train_keys_UZH = dicts_low['UZH'].keys()\n","print('Number of UZH sequences in train set (unlabeled) = {}'.format(len(train_keys_UZH)))\n","\n","# Broad-neutralizing sequences\n","train_keys_broad = dicts_low['broad'].keys()\n","print('Number of broad-neutr. sequences in train set = {}'.format(len(train_keys_broad)))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Number of non-neutr. sequences in train set (unlabeled) = 1369\n","Number of UZH sequences in train set (unlabeled) = 273\n","Number of broad-neutr. sequences in train set = 7079\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T8n3C5FMhoJz"},"source":["4. Create csv file"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GrEZg5G-hoJz","executionInfo":{"status":"ok","timestamp":1623682615445,"user_tz":-120,"elapsed":18,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"4f392165-026a-45ab-99aa-8d6ebc2124b3"},"source":["u_l_ratio = (len(train_keys_non_unlab)+len(train_keys_UZH)+len(train_keys_broad))/(len(train_keys_best)+len(train_keys_non_lab)+len(train_keys_UZH_lab))\n","balance = int(math.log(u_l_ratio, 2)) # Indicates how often the labeled instances will be repeated in train set\n","print('Ratio between unlabeled and labeled data is {}'.format(u_l_ratio))\n","print('Labeled sequences will be repeated {} times'.format(balance))\n","\n","with open(file_name_train+'.csv', mode='w') as csv_file:\n","    print('Creating file \"{}\"'.format(file_name_train+'.csv'))\n","    writer = csv.writer(csv_file, delimiter=',')\n","    writer.writerow(['seq-id', 'label', 'sequence', 'germline', 'label-mask'])\n","\n","    # Add positive instances (with augmentation)\n","    for k in train_keys_best:\n","        sa = best_dict[k]['sequence_alignment_aa']\n","        ga = best_dict[k]['germline_alignment_aa']\n","        for i in range(balance):    \n","            writer.writerow([k, 1, sa, ga, 1])\n","\n","    # Add negative instances (with augmentation)\n","    for k in train_keys_non_lab:\n","        sa = dicts_low['non'][k]['sequence_alignment_aa']\n","        ga = dicts_low['non'][k]['germline_alignment_aa']\n","        for i in range(balance):\n","            writer.writerow([k, 0, sa, ga, 1])\n","    \n","    # Add negative instances (with augmentation)\n","    for k in train_keys_UZH_lab:\n","        sa = dicts_high['UZH'][k]['sequence_alignment_aa']\n","        ga = dicts_high['UZH'][k]['germline_alignment_aa']\n","        for i in range(balance):\n","            writer.writerow([k, 0, sa, ga, 1])\n","    \n","    # Add unlabeled instances (broad)\n","    for k in train_keys_broad:\n","        sa = dicts_low['broad'][k]['sequence_alignment_aa']\n","        ga = dicts_low['broad'][k]['germline_alignment_aa']\n","        writer.writerow([k, 0, sa, ga, 0])\n","    \n","    # Add unlabeled instances (non)\n","    for k in train_keys_non_unlab:\n","        sa = dicts_low['non'][k]['sequence_alignment_aa']\n","        ga = dicts_low['non'][k]['germline_alignment_aa']\n","        writer.writerow([k, 0, sa, ga, 0])\n","    \n","    # Add UZH instances\n","    for k in train_keys_UZH:\n","        sa = dicts_low['UZH'][k]['sequence_alignment_aa']\n","        ga = dicts_low['UZH'][k]['germline_alignment_aa']\n","        writer.writerow([k, 0, sa, ga, 0])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Ratio between unlabeled and labeled data is 41.92788461538461\n","Labeled sequences will be repeated 5 times\n","Creating file \"TRAIN_SET_baseline+UZH_labeled_non.csv\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-DJ9CmwmTWTx"},"source":["## CREATE TESTING DATA"]},{"cell_type":"markdown","metadata":{"id":"SD1coFNBREAi"},"source":["Define parameters for test set(s)"]},{"cell_type":"code","metadata":{"id":"nwdJC_XNQUB3","executionInfo":{"status":"ok","timestamp":1623682615445,"user_tz":-120,"elapsed":15,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["file_name_test = 'TEST_SET'\n","file_name_val = 'VALIDATION_SET'\n","\n","# Ratio between postive and negative instances in baseline test set\n","test_pos_ratio = 1\n","\n","# Should be the same as for training set\n","alpha = 0.3\n","\n","# Size of other test sets\n","test_size = 1000 # sys.maxsize # if available"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"afDNTe58ubut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682615446,"user_tz":-120,"elapsed":16,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"de80a00f-ac09-4765-ff8e-ec569a21fe3f"},"source":["# Writer for validation set\n","csv_file_val = open(file_name_val+'.csv', mode='w')\n","writer_val = csv.writer(csv_file_val, delimiter=',')\n","writer_val.writerow(['seq-id', 'label', 'sequence', 'germline', 'label-mask'])\n","\n","# Writer for test set\n","csv_file_test = open(file_name_test+'.csv', mode='w')\n","writer_test = csv.writer(csv_file_test, delimiter=',')\n","writer_test.writerow(['seq-id', 'label', 'sequence', 'germline', 'label-mask'])"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["43"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"wV3_NLVVe_L6"},"source":["###### Baseline"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BEcdRIJgCnH","executionInfo":{"status":"ok","timestamp":1623682615447,"user_tz":-120,"elapsed":15,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"63f1b6c6-7218-4840-b50c-56b2182b3c19"},"source":["# Remove best keys used in training set:\n","train_keys = list(pd.read_csv(file_name_train+'.csv')['seq-id'])\n","train_keys_best = [x for x in train_keys if 'best' in x]\n","\n","test_keys_best = [k for k in best_dict.keys() if k not in train_keys_best]\n","\n","# Compute set of keys for negative instances of test set (most similar to positives ones)\n","given_dict = {k:v for k,v in best_dict.items() if k in test_keys_best}\n","test_keys_non = find_most_similar(given_dict, dicts_low_test['non'], int(len(test_keys_best)/test_pos_ratio), alpha)\n","# Check similarity between positive and negative instances of test set\n","avg_sim, min_sim, max_sim = check_similarity(test_keys_best, test_keys_non, best_dict, dicts_low_test['non'], alpha)\n","print('Similarity between positive and negative instances of test set: min =',min_sim,', avg =',avg_sim,'\\n')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Similarity between positive and negative instances of test set: min = 0.5036662452591656 , avg = 0.6281056090938586 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cjE4W_ApglwV","executionInfo":{"status":"ok","timestamp":1623682615447,"user_tz":-120,"elapsed":14,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["# Add positive instances\n","for k in test_keys_best:\n","    sa = best_dict[k]['sequence_alignment_aa']\n","    ga = best_dict[k]['germline_alignment_aa']\n","    writer_val.writerow([k, 1, sa, ga, 1])\n","    writer_test.writerow([k, 1, sa, ga, 1])\n","\n","# Add negative instances\n","for k in test_keys_non:\n","    sa = dicts_low_test['non'][k]['sequence_alignment_aa']\n","    ga = dicts_low_test['non'][k]['germline_alignment_aa']\n","    writer_val.writerow([k, 0, sa, ga, 1])\n","    writer_test.writerow([k, 0, sa, ga, 1])\n","\n","# Remove non-broad-neutr. sequences from dicts_low_test\n","dicts_low_test['non'] = {k:v for k,v in dicts_low_test['non'].items() if k not in test_keys_non}\n","\n","csv_file_val.close()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"On2kMN3YgX3k"},"source":["###### Others (below threshold)"]},{"cell_type":"code","metadata":{"id":"TblYdPIjgV9I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682615447,"user_tz":-120,"elapsed":13,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"c777e0dd-bfcc-48db-cf9f-b154dc03a8e6"},"source":["for test,_ in ds_list:\n","\n","    test_keys = list(dicts_low_test[test].keys())[:test_size]\n","    print('| {} | Number of test sequences: {}'.format(test,len(test_keys)))\n","\n","    for k in test_keys:\n","        sa = dicts_low_test[test][k]['sequence_alignment_aa']\n","        ga = dicts_low_test[test][k]['germline_alignment_aa']\n","        writer_test.writerow([k+'_low', 0, sa, ga, 1])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["| broad | Number of test sequences: 500\n","| non | Number of test sequences: 286\n","| UZH | Number of test sequences: 100\n","| simonich | Number of test sequences: 5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1NiVaSaWhB11"},"source":["###### Others (above threshold)"]},{"cell_type":"code","metadata":{"id":"bC7FvtpKjKSg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682615448,"user_tz":-120,"elapsed":13,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"fc4361ed-014a-4211-d6e5-3e7257d716b2"},"source":["for test,_ in ds_list:\n","\n","    test_keys = list(dicts_high_test[test].keys())[:test_size]\n","    print('| {} | Number of test sequences: {}'.format(test,len(test_keys)))\n","\n","    for k in test_keys:\n","        sa = dicts_high_test[test][k]['sequence_alignment_aa']\n","        ga = dicts_high_test[test][k]['germline_alignment_aa']\n","        writer_test.writerow([k+'_high', 0, sa, ga, 1])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["| broad | Number of test sequences: 1000\n","| non | Number of test sequences: 1000\n","| UZH | Number of test sequences: 1000\n","| simonich | Number of test sequences: 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"chLLBLwHYZ05","executionInfo":{"status":"ok","timestamp":1623682615996,"user_tz":-120,"elapsed":560,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["csv_file_test.close()"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QMgdcfmLww6b"},"source":["# Define the generator and discriminator"]},{"cell_type":"markdown","metadata":{"id":"8h1REdz1Fzno"},"source":["This part is borrowed from https://github.com/crux82/ganbert-pytorch/blob/main/GANBERT_pytorch.ipynb"]},{"cell_type":"code","metadata":{"id":"5KahSLeRw4Z-","executionInfo":{"status":"ok","timestamp":1623682624568,"user_tz":-120,"elapsed":352,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["#------------------------------\n","#   The Generator as in \n","#   https://www.aclweb.org/anthology/2020.acl-main.191/\n","#   https://github.com/crux82/ganbert\n","#------------------------------\n","class Generator(nn.Module):\n","    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n","        super(Generator, self).__init__()\n","        layers = []\n","        hidden_sizes = [noise_size] + hidden_sizes\n","        for i in range(len(hidden_sizes)-1):\n","            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n","\n","        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, noise):\n","        output_rep = self.layers(noise)\n","        return output_rep\n","\n","#------------------------------\n","#   The Discriminator\n","#   https://www.aclweb.org/anthology/2020.acl-main.191/\n","#   https://github.com/crux82/ganbert\n","#------------------------------\n","class Discriminator(nn.Module):\n","    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n","        super(Discriminator, self).__init__()\n","        self.input_dropout = nn.Dropout(p=dropout_rate)\n","        layers = []\n","        hidden_sizes = [input_size] + hidden_sizes\n","        for i in range(len(hidden_sizes)-1):\n","            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n","\n","        self.layers = nn.Sequential(*layers) #per il flatten\n","        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, input_rep):\n","        input_rep = self.input_dropout(input_rep)\n","        last_rep = self.layers(input_rep)\n","        logits = self.logit(last_rep)\n","        probs = self.softmax(logits)\n","        return last_rep, logits, probs"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KppC4MCYxF7n"},"source":["# Train and test"]},{"cell_type":"code","metadata":{"id":"KzEuhqOPGUld","executionInfo":{"status":"ok","timestamp":1623682625912,"user_tz":-120,"elapsed":5,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["class Antibody_dataset(Dataset):\n","    def __init__(self, file_name):\n","        self.data = pd.read_csv(file_name)\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        seqID = self.data['seq-id'][idx]\n","        lab = self.data['label'][idx]\n","        seq1 = self.data['sequence'][idx]\n","        seq2 = self.data['germline'][idx]\n","        m = self.data['label-mask'][idx]\n","\n","        return {'seq-id':seqID, 'label':lab, 'sequence':seq1, 'germline':seq2, 'label-mask':m}"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"srznwvmOMQcm","executionInfo":{"status":"ok","timestamp":1623682625913,"user_tz":-120,"elapsed":5,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["baseline_params = {\n","    'num_classes': 2,\n","    'hidden_size': 768,\n","    'noise_size': 100,\n","    'epsilon': 1e-8,\n","    'dropout_rate': 0.2,\n","\n","    'batch_size': 64,\n","    'max_epochs': 12,\n","    'learning_rate': 2e-5,\n","    \n","    'apply_schedular': False,\n","    'warmup_proportion': 0.1,\n","}\n","\n","num_classes = baseline_params['num_classes']\n","hidden_size = baseline_params['hidden_size']\n","noise_size = baseline_params['noise_size']\n","epsilon = baseline_params['epsilon']\n","dropout_rate = baseline_params['dropout_rate']\n","\n","batch_size = baseline_params['batch_size']\n","max_epochs = 8 # baseline_params['max_epochs']\n","learning_rate = baseline_params['learning_rate']\n","\n","apply_schedular = baseline_params['apply_schedular']\n","warmup_proportion = baseline_params['warmup_proportion']\n","\n","log_interval = 5 # During training, losses will be recored each log_interval batches\n","use_pretrained = True # Whether to train or use pretrained models\n","model_dir = '/content/drive/MyDrive/models/experiment-c/' # Where to store (& load) models (BERT ~ 350 MB). I used google drive.\n","val_interval = 5 # During training, tests on val. set will be run each val_interval batches\n","\n","# Description of current run \n","train_set = file_name_train.replace('TRAIN_SET_','')\n","train_params = 'baseline'\n","file_names = {\n","    'bert': 'BERT_trained_on=['+train_set+']_params=['+train_params+'].pt',\n","    'discriminator': 'DISCRIMINATOR_trained_on=['+train_set+']_params=['+train_params+'].pt',\n","    'stats': 'STATISTICS_trained_on=['+train_set+']_params=['+train_params+']',\n","    'pretrained_stats': 'STATISTICS_trained_on=['+train_set+']_params=['+train_params+']_using_pretrained'\n","}"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CITB1oYx47l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682670438,"user_tz":-120,"elapsed":44529,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"729d7556-321d-401b-c4bb-dc97030efc32"},"source":["# Define models\n","bert = ProteinBertModel.from_pretrained('bert-base')\n","generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=[hidden_size], dropout_rate=dropout_rate)\n","discriminator = Discriminator(input_size=hidden_size, hidden_sizes=[hidden_size], num_labels=num_classes, dropout_rate=dropout_rate)\n","\n","# Tell pytorch to run the models on the selected device\n","bert.to(device)\n","generator.to(device)\n","discriminator.to(device)\n","\n","bert_vars = [i for i in bert.parameters()]\n","d_vars = bert_vars + [v for v in discriminator.parameters()]\n","g_vars = [g for g in generator.parameters()]\n","\n","# Define optimizers\n","d_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate, eps=1e-6)\n","g_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate, eps=1e-6)\n","\n","# Define (pretrained) tokenizer\n","tokenizer = TAPETokenizer(vocab='iupac')\n","\n","# Datasets\n","ds_train = Antibody_dataset(file_name_train+'.csv')\n","dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True) #, num_workers=4)\n","no_bat_train = len(dl_train)\n","\n","ds_test = Antibody_dataset(file_name_test+'.csv')\n","dl_test = DataLoader(ds_test, batch_size=batch_size) #, num_workers=4)\n","# no_bat_test = len(dl_test)\n","\n","ds_val = Antibody_dataset(file_name_val+'.csv')\n","dl_val = DataLoader(ds_val, batch_size=batch_size) #, num_workers=4)\n","# no_bat_val = len(dl_val)\n","\n","# Metrics\n","accuracy = Accuracy()\n","precision = Precision(average=None, num_classes=num_classes)\n","recall = Recall(average=None, num_classes=num_classes)\n","f1 = F1(average=None, num_classes=num_classes)\n","\n","if apply_schedular:\n","    total_steps = no_bat_train * max_epochs\n","    warmup_steps = int(total_steps * warmup_proportion)\n","\n","    # Define learning rate schedulars with built-in warmup\n","    d_scheduler = get_polynomial_decay_schedule_with_warmup(d_optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps,lr_end=0,power=1)\n","    g_scheduler = get_polynomial_decay_schedule_with_warmup(g_optimizer,num_warmup_steps=warmup_steps,num_training_steps=total_steps,lr_end=0,power=1)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["100%|██████████| 567/567 [00:00<00:00, 187036.60B/s]\n","100%|██████████| 370264230/370264230 [00:36<00:00, 10175322.76B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Rk0zJNydzSNJ","executionInfo":{"status":"ok","timestamp":1623682670440,"user_tz":-120,"elapsed":10,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}}},"source":["def run_test(dataloader,training_batch=0,verbose=False):\n","    \n","    if verbose:\n","        print('Testing...(num. of batches = {})'.format(len(dataloader)))\n","\n","    t0 = time.time()\n","\n","    bert.eval()\n","    generator.eval()\n","    discriminator.eval()\n","\n","    global all_data; all_data = dict()\n","\n","    for batch in dataloader:\n","\n","        seq_list = batch['sequence']\n","\n","        # Needed for padding purposes\n","        max_len_seq = max([len(x) for x in seq_list])\n","\n","        # Tokenize and pad to uniform size (to be able to create tensor)\n","        seq_ids = torch.tensor([list(tokenizer.encode(x))+[0]*(max_len_seq-len(x)) for x in seq_list], device=device)\n","        seq_attention_masks = torch.tensor([[1]*(len(x)+2)+[0]*(max_len_seq-len(x)) for x in seq_list], device=device)\n","\n","        with torch.no_grad():\n","\n","            bert_out_seq = bert(seq_ids, seq_attention_masks)[1]\n","            bert_output = bert_out_seq\n","\n","            D_real_features, D_real_logits, D_real_prob = discriminator(bert_output)\n","            logits = D_real_logits[:, 1:]\n","        \n","        # probabilities = torch.nn.functional.softmax(logits, dim=-1)\n","        _, preds = torch.max(logits, dim=1)\n","        \n","        labels = batch['label'].tolist()\n","        preds = preds.detach().cpu().tolist()\n","        logits = logits.detach().cpu().tolist()\n","\n","        # Accumulate ids, predictions, labels and logits\n","        for i, k in enumerate(batch['seq-id']):\n","            all_data[k] = [labels[i],preds[i],logits[i]]\n","    \n","    save_obj(all_data,'all_data_experiment_C')\n","\n","    for ger_type in ['_low','_high']:\n","        for test in ['broad','non','UZH','simonich']:\n","            current_keys = [x for x in all_data.keys() if ger_type in x and test in x]\n","\n","            # Percentages of negative and postive predictions\n","            neg_pred = len([x for x in current_keys if all_data[x][1] == 0])/max(len(current_keys),1)\n","            pos_pred = len([x for x in current_keys if all_data[x][1] == 1])/max(len(current_keys),1)\n","            \n","            test_stats.setdefault(test+ger_type+'_%_neg',[]).append(neg_pred)\n","            test_stats.setdefault(test+ger_type+'_%_pos',[]).append(pos_pred)\n","            \n","            # Remove those already seen\n","            for k in current_keys:\n","                all_data.pop(k)\n","\n","    # Baseline labels and predictions\n","    all_labels = torch.tensor([x[1][0] for x in all_data.items()])\n","    all_preds = torch.tensor([x[1][1] for x in all_data.items()])\n","\n","    acc = accuracy(all_preds, all_labels)\n","    pre = precision(all_preds, all_labels)\n","    rec = recall(all_preds, all_labels)\n","    f1_ = f1(all_preds, all_labels)\n","\n","    # Compute loss on baseline set (cross-entropy)\n","    logits = torch.tensor([x[1][2] for x in all_data.items()], dtype=torch.double)\n","    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","    one_hot_labels = torch.nn.functional.one_hot(all_labels, num_classes=num_classes)\n","    per_example_loss =  - torch.sum(one_hot_labels * log_probs, dim=-1)\n","    disc_loss_supervised = torch.mean(per_example_loss)\n","\n","    test_stats.setdefault('sample points',[]).append(training_batch)\n","    test_stats.setdefault('accuracy',[]).append(acc.item())\n","    test_stats.setdefault('precision_neg',[]).append(pre[0].item())\n","    test_stats.setdefault('precision_pos',[]).append(pre[1].item())\n","    test_stats.setdefault('recall_neg',[]).append(rec[0].item()) # Specificity\n","    test_stats.setdefault('recall_pos',[]).append(rec[1].item())\n","    test_stats.setdefault('f1_neg',[]).append(f1_[0].item())\n","    test_stats.setdefault('f1_pos',[]).append(f1_[1].item())\n","    test_stats.setdefault('validation loss',[]).append(disc_loss_supervised.item())\n","    test_stats.setdefault('validation time',[]).append(format_time(time.time()-t0))\n","\n","    if verbose:\n","        for k in test_stats.keys():\n","            print(k+' = '+str(test_stats[k][-1]), end=' ')\n","        print()\n","        \n","    return disc_loss_supervised.item()"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Klx6nNLZGQT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623682700946,"user_tz":-120,"elapsed":30515,"user":{"displayName":"Thomas Fraling","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqwbcI4q-4VbyT-O2pKVANPbvG1RovdPFACEGNhQ=s64","userId":"13837266948653238773"}},"outputId":"dc4874ad-ef31-4574-9569-0545a5a3558a"},"source":["torch.manual_seed(seed_val) # Just to be sure\n","\n","# Gather data and store in results file\n","train_stats = dict()\n","test_stats = dict()\n","\n","if use_pretrained:\n","    # Load trained models. Generator is not used at testing time\n","    bert.load_state_dict(torch.load(model_dir+file_names['bert']))\n","    discriminator.load_state_dict(torch.load(model_dir+file_names['discriminator']))\n","\n","    run_test(dl_test,verbose=True)\n","    stats = {\n","        'batch_size': batch_size,\n","        'learning_rate': learning_rate,\n","        # 'num_batches_train': len(dl_train),\n","        # 'best_val_loss': best_val_loss,\n","        # 'best_batch': best_batch,\n","        # 'train_stats': train_stats,\n","        'test_stats': test_stats\n","    }\n","    save_obj(stats,file_names['pretrained_stats'])\n","\n","else:\n","\n","    best_val_loss = sys.maxsize; best_batch = 1\n","    best_bert_dict = dict(); best_disc_dict = dict()\n","\n","    for epoch_i in range(max_epochs):\n","\n","        print('Training... (num. of batches = {})'.format(no_bat_train))\n","\n","        bert.train()\n","        generator.train()\n","        discriminator.train()\n","\n","        epoch_stats = torch.empty((6,0), device=device)\n","        sample_points = list()\n","\n","        t0 = time.time() \n","\n","        for idx, batch in enumerate(dl_train):\n","            \n","            t0_bat = time.time()\n","\n","            ## 0. Prepare data\n","\n","            seq_list = batch['sequence']\n","\n","            # Needed for padding purposes\n","            max_len_seq = max([len(x) for x in seq_list])\n","\n","            # Tokenize and pad to uniform size (to be able to create tensor)\n","            seq_ids = torch.tensor([list(tokenizer.encode(x))+[0]*(max_len_seq-len(x)) for x in seq_list], device=device)\n","            seq_attention_masks = torch.tensor([[1]*(len(x)+2)+[0]*(max_len_seq-len(x)) for x in seq_list], device=device)\n","            \n","            labels = batch['label'].to(device)\n","            label_mask = batch['label-mask'].to(device).bool()\n","\n","            noise = torch.rand((seq_ids.size()[0], noise_size), device=device)\n","            generator_output = generator(noise)\n","\n","            ## 1. Discriminator losses\n","            \n","            ## 1.1 Supervised loss\n","            \n","            # Use representation of CLS token as the sentence embedding. Vector has length 768\n","            bert_out_seq = bert(seq_ids, seq_attention_masks)[1]\n","\n","            bert_output = bert_out_seq\n","            \n","            D_real_features, D_real_logits, D_real_prob = discriminator(bert_output)\n","            D_fake_features, DU_fake_logits, DU_fake_prob = discriminator(generator_output)\n","\n","            logits = D_real_logits[:, 1:]\n","            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n","\n","            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","\n","            one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=num_classes)\n","\n","            per_example_loss =  - torch.sum(one_hot_labels * log_probs, dim=-1)\n","            per_example_loss = torch.masked_select(per_example_loss, label_mask)\n","            \n","            labeled_example_count = torch.tensor(per_example_loss.size()[0], dtype=torch.float32)\n","\n","            disc_loss_supervised = torch.div(torch.sum(per_example_loss), torch.max(torch.tensor(1), labeled_example_count.long()))\n","\n","            ## 1.2 Unsupervised losses\n","            \n","            disc_loss_unsupervised = - torch.mean(torch.log(1 - D_real_prob[:, 0] + epsilon))\n","            \n","            disc_loss_generator = - torch.mean(torch.log(DU_fake_prob[:, 0] + epsilon))\n","\n","            ## 2. Generator losses\n","\n","            ## 2.1 Use generator output\n","\n","            gen_loss_generator = - torch.mean(torch.log(1 - DU_fake_prob[:, 0] + epsilon))\n","\n","            ## 2.2 Feature matching\n","\n","            tmp = torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0)\n","            gen_loss_feat_match = torch.mean(torch.pow(tmp, 2)) # tmp * tmp)\n","\n","            # 3. Optimization\n","\n","            g_loss = gen_loss_generator + gen_loss_feat_match\n","            d_loss = disc_loss_supervised + disc_loss_unsupervised + disc_loss_generator\n","\n","            g_optimizer.zero_grad()\n","            d_optimizer.zero_grad()\n","\n","            g_loss.backward(retain_graph=True)\n","            d_loss.backward()\n","            \n","            g_optimizer.step()\n","            d_optimizer.step()\n","\n","            if apply_schedular:\n","                g_scheduler.step()\n","                d_scheduler.step()\n","\n","            ## 4. Logging\n","            if idx % log_interval == 0 or idx == no_bat_train-1:\n","                tmp = torch.tensor((\n","                    disc_loss_supervised.detach(),\n","                    disc_loss_unsupervised.detach(),\n","                    disc_loss_generator.detach(),\n","                    gen_loss_generator.detach(),\n","                    gen_loss_feat_match.detach(),\n","                    labeled_example_count.detach()\n","                    ),device=device)\n","                tmp.resize_((6, 1))\n","                epoch_stats = torch.cat((epoch_stats,tmp),1)\n","                sample_points.append((no_bat_train*epoch_i)+(idx+1))\n","            \n","            if idx % val_interval == 0:\n","                curr_val_loss = run_test(dl_val,(no_bat_train*epoch_i)+(idx+1))\n","                if curr_val_loss < best_val_loss:\n","                    best_val_loss = curr_val_loss\n","                    best_batch = (no_bat_train*epoch_i)+(idx+1)\n","\n","                    # run_test(dl_test,(no_bat_train*epoch_i)+(idx+1))\n","\n","                    # Save models when val. loss is minimal\n","                    torch.save(bert.state_dict(), model_dir+file_names['bert'])\n","                    torch.save(discriminator.state_dict(), model_dir+file_names['discriminator'])\n","\n","            t1_bat = time.time()\n","            sys.stdout.write('\\r| Started {} | Epoch {} / {} | Progress {} % | Last batch took {} sec. |'\\\n","                                .format(format_time(t0+7200),epoch_i+1,max_epochs,int(100*(idx+1)/no_bat_train),round(t1_bat-t0_bat,4)))\n","            sys.stdout.flush()\n","\n","        dt = format_time(time.time()-t0)\n","        train_stats.setdefault('sample points',[]).extend(sample_points)\n","        train_stats.setdefault('supervised loss (D)',[]).extend(epoch_stats[0,:].tolist())\n","        train_stats.setdefault('unsupervised loss 1 (D)',[]).extend(epoch_stats[1,:].tolist())\n","        train_stats.setdefault('unsupervised loss 2 (D)',[]).extend(epoch_stats[2,:].tolist())\n","        train_stats.setdefault('unsupervised loss (G)',[]).extend(epoch_stats[3,:].tolist())\n","        train_stats.setdefault('feature matching loss (G)',[]).extend(epoch_stats[4,:].tolist())\n","        train_stats.setdefault('labeled count',[]).extend(epoch_stats[5,:].tolist())\n","        # train_stats.setdefault('duration',[]).append(dt\n","\n","        print('\\n| Epoch',epoch_i+1,'/',max_epochs,'| Avg. losses = ',[sum(x)/len(x) for x in epoch_stats[:-1].tolist()])\n","        print('| Epoch',epoch_i+1,'/',max_epochs,'| Training time =',dt,'\\n')\n","\n","        run_test(dl_val,(no_bat_train*epoch_i)+(idx+1),verbose=True)\n","        \n","        stats = {\n","            'batch_size': batch_size,\n","            'learning_rate': learning_rate,\n","            'num_batches_train': len(dl_train),\n","            'best_val_loss': best_val_loss,\n","            'best_batch': best_batch,\n","            'train_stats': train_stats,\n","            'test_stats': test_stats\n","        }\n","        save_obj(stats,file_names['stats'])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Testing...(num. of batches = 77)\n","broad_low_%_neg = 0.98 broad_low_%_pos = 0.02 non_low_%_neg = 0.9825174825174825 non_low_%_pos = 0.017482517482517484 UZH_low_%_neg = 0.29 UZH_low_%_pos = 0.71 simonich_low_%_neg = 0.2 simonich_low_%_pos = 0.8 broad_high_%_neg = 0.991 broad_high_%_pos = 0.009 non_high_%_neg = 0.993 non_high_%_pos = 0.007 UZH_high_%_neg = 0.647 UZH_high_%_pos = 0.353 simonich_high_%_neg = 0.693 simonich_high_%_pos = 0.307 sample points = 0 accuracy = 0.8928571343421936 precision_neg = 0.8235294222831726 precision_pos = 1.0 recall_neg = 1.0 recall_pos = 0.7857142686843872 f1_neg = 0.9032257795333862 f1_pos = 0.8799999952316284 validation loss = 0.3958691005007981 validation time = 00:00:20 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a0RCc1tl8_jk"},"source":["# Visualize results"]}]}